% !TeX program = xelatex
\documentclass[11pt,aspectratio=169]{beamer}

\usetheme{metropolis}
\usepackage{fontspec}
\usepackage{appendixnumberbeamer}
\usepackage{graphicx,booktabs,amsmath,amssymb,tikz}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{mathtools} 
\usepackage{colortbl} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pifont}
\usepackage[backend=biber]{biblatex}
\addbibresource{EAP_ML.bib}
% ---------------- Fonts ----------------
\setsansfont{Fira Sans}[
  Ligatures=TeX,
  Scale=MatchLowercase,
  UprightFont     = * Book,
  ItalicFont      = * Book Italic,
  BoldFont        = * SemiBold,
  BoldItalicFont  = * SemiBold Italic
]
\setmonofont{Fira Mono}[Scale=MatchLowercase]

\title{\textsc{Empirical Asset Pricing via ML in the EU}}
\author{Lucas DUBOIS \& Myriam LAMBORELLE}
\institute{HEC LiÃ¨ge}
\date{\today}

% ---------------- Executive Palette ----------------
\definecolor{primaryred}{RGB}{127,20,22}
\definecolor{midnight}{RGB}{0,22,36}
\definecolor{steelblue}{RGB}{171,193,223}
\definecolor{harmonizedblue}{RGB}{48,88,140}
\definecolor{offwhite}{RGB}{255,255,255}
\definecolor{darkgreen}{RGB}{34,139,34}

% ---------------- Items Color ----------------
\setbeamercolor{enumerate item}{fg=harmonizedblue}
\setbeamercolor{itemize item}{fg=harmonizedblue}

\setbeamercolor{background canvas}{bg=offwhite}
\setbeamercolor{normal text}{fg=midnight}
\setbeamercolor{title}{fg=midnight}
\setbeamercolor{subtitle}{fg=midnight!80}
\setbeamercolor{author}{fg=midnight!75}
\setbeamercolor{date}{fg=midnight!75}
\setbeamercolor{frametitle}{fg=steelblue,bg=midnight}

% Progress bar colors (used on section pages)
\setbeamercolor{progress bar}{fg=primaryred,bg=midnight!30}
\setbeamercolor{progress bar in section page}{fg=primaryred,bg=midnight!30}
\setbeamercolor{title separator}{fg=primaryred,bg=primaryred!25}
% Blocks very light (demo-style)
\setbeamercolor{block title}{fg=midnight,bg=steelblue!12}
\setbeamercolor{block body}{fg=midnight,bg=steelblue!6}

% ---------------- Metropolis options ----------------
% Show ONLY a section page between sections with a progress bar.
% No per-frame progress under the frametitle.
\metroset{
  numbering=fraction,
  progressbar=none,          % no frametitle progress bar
  sectionpage=progressbar,   % section pages with progress bar (single line)
  block=transparent,
  titleformat title=regular,
  subsectionpage=none
}

% Make the section-page progress bar thicker
\makeatletter
\setlength{\metropolis@progressonsectionpage@linewidth}{1.8pt} % thicker than default
\makeatother

\hypersetup{colorlinks=true,linkcolor=primaryred,urlcolor=primaryred}

\begin{document}

% ========== Title page ==========
\maketitle

% ========== Outline ==========
\begin{frame}{\textsc{Table of Contents}}
  \vspace{1.0ex}
  \tableofcontents
\end{frame}

% ================== Content ==================
\section{\textsc{Introduction}}

%=================HERE FOR THE NEW PART=================

\begin{frame}{\textsc{Database}}
  \begin{itemize}
      \item {\color{primaryred}\textbf{Universe:}} Monthly european stocks (Refinitiv).
      \item {\color{primaryred}\textbf{Sample:}} {\color{steelblue}\textbf{599 individual stocks}} (stoxx600 constituents).
      \item {\color{primaryred}\textbf{$R_f$} :} EURIBOR 1M.
      \item {\color{primaryred}\textbf{Period:}}
         {\color{harmonizedblue}\textbf{
$\underbrace{2005\text{--}2015}_{\text{Train}}
\;\;|\;\;
\underbrace{2016\text{--}2025}_{\text{Test}}$
}}
      \item {\color{primaryred}\textbf{Predictors:}} {\textbf{ 10 firm-level characteristics}}  + {\textbf{12 interaction terms.}} 
  \end{itemize}
\end{frame}

\section{\textsc{Data Preprocessing}}

\begin{frame}{\textsc{Winsorization and Standardization}}
  \textcolor{primaryred}{GKN} \cite{gu_empirical_2020} do not explicitly describe how firm characteristics are preprocessed, so we follow {\color{primaryred}Drobetz and Otto} \cite{drobetz_empirical_2020} and implement a \textcolor{harmonizedblue}{winsorization} procedure.
  \medskip
\begin{center}
  \textit{Values below the lower threshold are replaced by the maximum of the lower tail (\textcolor{primaryred}{1st-percentile} ), and values above the upper threshold are replaced by the minimum of the upper tail (\textcolor{steelblue}{99th-percentile}). Predictors are then standardized using training-sample moments.}
\end{center}
  \medskip
  \begin{itemize}
      \item[\textcolor{darkgreen}{\ding{51}}] Limits influence of outliers.
      \item[\textcolor{darkgreen}{\ding{51}}] Ensures comparable scale across predictors.
      \item[\textcolor{darkgreen}{\ding{51}}] Improves out-of-sample performance.
  \end{itemize}
  \end{frame}

  \begin{frame}{\textsc{Winsorization and Standardization}}
    \begin{algorithm}[H]
      \small
      \caption{{\color{primaryred}Cross-Sectional Standardizer}}
      \begin{algorithmic}
        \STATE \textbf{Input:} Training data $\mathcal{D}^{\text{train}}$, data $\mathcal{D}$, predictors $\mathcal{X}$
        
        \FOR{each predictor $j \in \mathcal{X}$}
          \STATE Compute lower quantile $q_j^{L}$ and upper quantile $q_j^{U}$ from $\mathcal{D}^{\text{train}}$
          \STATE Compute mean $\mu_j$ and standard deviation $\sigma_j$ from $\mathcal{D}^{\text{train}}$
        \ENDFOR
        
        \FOR{each observation $i \in \mathcal{D}$}
          \FOR{each predictor $j \in \mathcal{X}$}
            \STATE $X_{ij} \leftarrow \min(\max(X_{ij}, q_j^{L}), q_j^{U})$
            \STATE $\tilde{X}_{ij} \leftarrow (X_{ij} - \mu_j) / \sigma_j$
          \ENDFOR
        \ENDFOR
            \end{algorithmic}
    \end{algorithm}
  \end{frame}

  \begin{frame}{\textsc{Cross-Validation for Tuning Parameters:}}
    The authors use a \textbf{\color{steelblue} rolling one-step-ahead out-of-sample cross-validation} procedure to select the \textbf{\color{primaryred}hyperparameters} of their models:
      \begin{align}
        {\color{primaryred}(\lambda^\ast,\rho^\ast)}
        &= \arg\min_{\lambda,\rho}
        \sum_{t=T_0}^{T-1}
        \sum_{i=1}^{N_t}
        \bigl( r_{i,t+1} - \hat r_{i,t+1}(\lambda,\rho) \bigr)^2 .
        \end{align}    
    This allows to:
    \begin{enumerate}
      \item Account for \textbf{\color{steelblue} time-series dependence} in financial data.
      \item Mimic how \textbf{\color{steelblue} investors} would forecast their portfolios.
      \item Prevent \textbf{\color{steelblue} look-ahead bias} (terrible idea in finance!).
    \end{enumerate}
    \end{frame}

    \begin{frame}{\textsc{Sharpe Ratio from GKN}}
GKN \cite{gu_empirical_2020} propose to evaluate model performance using the \textbf{\color{steelblue}Sharpe ratio} of a long-short portfolio based on model predictions:
      \begin{align*}
        \boxed{
        {\color{harmonizedblue}SR^{*}}
        =
        \sqrt{
        \frac{
        SR^{2} + {\color{primaryred}R^{2}_{\text{OOS}}}
        }{
        1 - {\color{primaryred}R^{2}_{\text{OOS}}}
        }
        }
        }
        \end{align*}
        
      
      
      \vspace{0.5em}
      
      \begin{itemize}
        \item[\textcolor{darkgreen}{\ding{51}}] {\color{harmonizedblue}$SR^*$}: Adjusted Sharpe ratio using ${\color{primaryred}R^{2}_{\text{OOS}}}$.
        \item[\textcolor{darkgreen}{\ding{51}}] Rewards models with {\color{steelblue}high out-of-sample $R^2$}.
      \end{itemize}
      
      \end{frame}

  \begin{frame}{\textsc{Look-Ahead Bias Checks}}    
    \begin{enumerate}
        \item {\color{primaryred}Placebo Test}
        \begin{itemize}
            \item[\textcolor{darkgreen}{\ding{51}}] Replace model predictions with random noise.
            \item[\textcolor{darkgreen}{\ding{51}}] Verifies that portfolio construction alone does not generate spurious profits.
        \end{itemize}
    
        \item {\color{primaryred}Portfolio Cutoff Robustness}
        \begin{itemize}
            \item[\textcolor{darkgreen}{\ding{51}}] Vary the fraction of stocks in long and short portfolios.
            \item[\textcolor{darkgreen}{\ding{51}}] Verifies if results don't "break" with different cutoffs.
        \end{itemize}
    
        \item {\color{primaryred}Shifted Sharpe}
        \begin{itemize}
            \item[\textcolor{darkgreen}{\ding{51}}] Shift predicted signals backward in time.
            \item[\textcolor{darkgreen}{\ding{51}}] Confirms that results do not rely on forward-looking information.
        \end{itemize}
    \end{enumerate}

    \end{frame}
    
\section{\textsc{Models}}

\begin{frame}{\textsc{Models Tested}}
  \begin{itemize}
    \item[\textcolor{darkgreen}{\ding{51}}]OLS (\textcolor{primaryred}{benchmark})
    \item[\textcolor{darkgreen}{\ding{51}}]OLS-3 (\textcolor{primaryred}{benchmark})
    \item[\textcolor{darkgreen}{\ding{51}}] Elastic-Net (ENet)
    \item[\textcolor{darkgreen}{\ding{51}}] Random Forest (RF)
    \item[\textcolor{darkgreen}{\ding{51}}] Neural Networks (NN1 to NN6)
    \item[\textcolor{darkgreen}{\ding{51}}] Principal Component Regression (PCR) 
    \item[\textcolor{red}{\ding{55}}] Partial Least Squares (PLS)
    \item[\textcolor{red}{\ding{55}}] GBRT 
  \end{itemize}
\end{frame}

\begin{frame}{\textsc{Some Specifics}}
  \begin{enumerate}
    \item \textbf{\color{primaryred}Random Forest (RF):}
    \begin{itemize}
      \item \textcolor{harmonizedblue}{Depth:} 2,4,6
      \item \textcolor{harmonizedblue}{Leaf Size:} 40 
      \item \textcolor{harmonizedblue}{Number of trees:} 200
    \end{itemize}

    \item \textbf{\color{primaryred}Neural Networks (NNW):}
    \begin{itemize}
      \item \textcolor{harmonizedblue}{Depth:} 1-6 hidden layers
      \item \textcolor{harmonizedblue}{Epochs:} 50
      \item \textcolor{harmonizedblue}{Learning Rate(lr):} .001
    \end{itemize}

    \item \textbf{\color{primaryred}Principal Component Regression (PCR):}
    \begin{itemize}
      \item \textcolor{harmonizedblue}{Number of Components (K):} 1-6 (where 0 = OLS)
      \item \textcolor{harmonizedblue}{K:} Top eigenvectors (by variance explained)
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}{\textsc{The Huber Loss Function}}

  Following GKN \cite{gu_empirical_2020}, we use a \textbf{\color{steelblue}Huber loss function} to be robust to outliers when estimating our OLS benchmark models:
   \begin{align}
     {\color{primaryred}L_\delta(u)}=
     \begin{cases}
       \frac{1}{2} u^2, & \text{if } |u|\le {\color{steelblue}\delta}, \\
       \delta\left(|u|-\frac{1}{2}\delta\right), & \text{if } |u|>{\color{steelblue}\delta}.
     \end{cases}
   \end{align}  
 \medskip
 This allows:
 \begin{itemize}
   \item[\textcolor{darkgreen}{\ding{51}}] To control the influence of \textcolor{harmonizedblue}{extreme return observations}.
   \item[\textcolor{darkgreen}{\ding{51}}] Avoid the \textcolor{harmonizedblue}{sensitivity of MSE} to fat tails in return distributions.
 \end{itemize}
 \end{frame}

 \section{\textsc{Results \& Discussion}}

 \begin{frame}{$R_{OOS}^2$ \textsc{BEST Models:}}

  \centering
  \Large
  \begin{tabular}{lccccc}
  \toprule
   & {\color{harmonizedblue}NN5} 
   & {\color{harmonizedblue}NN4} 
   & {\color{harmonizedblue}RF} 
   & {\color{harmonizedblue}OLS-3} 
   & {\color{harmonizedblue}ENet} \\
  \midrule
  \rowcolor{green!20}
  {\color{primaryred}GKN}        
    & 0.36 & 0.39 & 0.33 & 0.26 & 0.16 \\
  \midrule
  \rowcolor{green!20}
  {\color{primaryred}ML} 
    & 1.50 & 1.20 & 1.10 & 0.93 & 0.80 \\
  \bottomrule
  \end{tabular}

\end{frame}

\begin{frame}{$R_{OOS}^2$ \textsc{WORST Models:}}

  \centering
  \Large
  \begin{tabular}{lccccc}
  \toprule
   & {\color{harmonizedblue}NN1} 
   & {\color{harmonizedblue}NN2} 
   & {\color{harmonizedblue}NN3} 
   & {\color{harmonizedblue}OLS} 
   & {\color{harmonizedblue}PCR} \\
  \midrule
  \rowcolor{red!20}
  {\color{primaryred}GNK}          
    & 0.33 & 0.39 & 0.40 & -3.46 & 0.26 \\
  \midrule
  \rowcolor{red!20}
  {\color{primaryred}ML} 
    & -18.10 & 0.70 & 0.8 & 0.68 & 0.80  \\
  \bottomrule
  \end{tabular}

\end{frame}

\begin{frame}{ \textsc{Sharpe BEST Models:}}

  \centering
  \LARGE
  \begin{tabular}{lcccc}
  \toprule
   & {\color{harmonizedblue}OLS} 
   & {\color{harmonizedblue}NN6} 
   & {\color{harmonizedblue}RF} 
   & {\color{harmonizedblue}NN4}\\
  \midrule
  \rowcolor{green!20}
  {\color{primaryred}GKN}        
    & LOW & -- & 0.98 & 1.35  \\
  \midrule
  \rowcolor{green!20}
  {\color{primaryred}ML} 
    & 1.03 & 0.87 & 0.73 & 0.73  \\
  \bottomrule
  \end{tabular}

\end{frame}

\begin{frame}{\textsc{Sharpe WORST Models:}}

  \centering
  \LARGE
  \begin{tabular}{lcccc}
  \toprule
   & {\color{harmonizedblue}NN2} 
   & {\color{harmonizedblue}NN3} 
   & {\color{harmonizedblue}NN5} 
   & {\color{harmonizedblue}OLS-3} \\
  \midrule
  \rowcolor{red!20}
  {\color{primaryred}GNK}          
    & 1.16 & 1.20 & 1.15 & 0.26 \\
  \midrule
  \rowcolor{red!20}
  {\color{primaryred}ML} 
    & 0.40 & 0.61 & 0.68 & 0.72   \\
  \bottomrule
  \end{tabular}

\end{frame}

\begin{frame}{\textsc{Cummulative Wealth Plot:}}
  \begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{/Users/lucasdubois/Desktop/LaTeX/EAP-ML/CODE/Images/cum_plot.png}
    \label{fig:cummulative_wealth}
  \end{figure}
\end{frame}

\begin{frame}{\textsc{Discussion}}
  \begin{enumerate}
    \item ${\color{harmonizedblue}R^2_{OOS}(ML)} > {\color{harmonizedblue}R^2_{OOS}(GKN)}$ but ${\color{harmonizedblue}SR^*(ML)} < {\color{harmonizedblue}SR^*(GKN)}$
    \begin{flushright}
      \textsc{\color{steelblue} ... Why?}
    \end{flushright}
    \item Different ${\color{steelblue}R_f}$ may still undermine results, but also european stocks have \textbf{\color{primaryred}lower returns} than US stocks.
    \item Is ML worthy under \textbf{\color{primaryred}relatively small databases }?
    \item \textcolor{harmonizedblue}{NNW}: VERY \textbf{\color{primaryred}powerfull!}
  \end{enumerate}
  
\end{frame}

\begin{frame}[allowframebreaks]{\textsc{References:}}
  \tiny
  \printbibliography[heading=none]
\end{frame}
{
\setbeamercolor{background canvas}{bg=midnight}
\begin{frame}[plain]
  \centering
  {\color{primaryred}\fontsize{150}{150}\selectfont ?}
\end{frame}
}
\end{document}
